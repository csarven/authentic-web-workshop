# Authentic Web mini-workshop series: Meeting 3, trustnet

Jun 10, 2025
[Reading Materials](https://github.com/w3c/authentic-web-workshop/issues/21)

<https://trustnet.csail.mit.edu/>

<https://www.youtube.com/watch?v=nTaAKbK6nXg>

<https://youtu.be/EKgMiJVa8Ew>

Video record: https://customer-0kix77mxh2zzzae0.cloudflarestream.com/5a540879881227aad6905e6a6f22a6c1/watch

Chat: irc.w3.org #CredWeb

Chair: Tzviya Siegman

Scribe: [Dominique Hazael-Massieux](mailto:dom@w3.org)

Present:

1. David Karger, trustnet
2. Scott Yates, (trust.txt)
3. Leonard Rosenthol (Adobe & C2PA)
4. François Daoust (W3C)
5. [Sarven Capadisli](https://csarven.ca/#i) ( <https://dokie.li/> , W3C [TAG](https://www.w3.org/2001/tag/))
6. Dominique Hazael-Massieux (W3C)
7. Virginia Balseiro (<https://dokie.li>)
8. Chris Needham (BBC)
9. Jeffrey Yasskin (Google Chrome)
10. Ehsan Toreini (Samsung)
11. Jennifer Strickland (MITRE)
12. Rick Byers (Google Chrome)
13. Annalee Flower Horne (Google Chrome)
14. Farnaz Jahanbakhsh (University of Michigan)
15. Sebastian Posth (Liccium.com)
16. Tzviya Siegman (W3C)
17. Benjamin Young (Digital Bazaar, Inc.)
18. Rob Smith (Away Team Software)
19. Barrett Golding (iffy.news)
20. Henrique Xavier
21. Franck Michel
22. Minoya (Sony)
23. Xiaojun Gu
24. Annette Greiner (Lawrence Berkeley National Lab)
25. Daveed Benjamin
26. [TallTed // Ted Thibodeau Jr](https://github.com/TallTed) (he/him) ([OpenLink Software](https://openlinksw.com/)) (at :37)


David: I'm a CSAIL professor at MIT with a research area on HCI, spent the 5-6 years working with Farnaz (now at Michigan U) on the TrustNet project; in this project, we're focusing on the role of trust in assessing information, which current online platforms mostly ignore. Empowering individuals and organizations to make their own choices on sources they trust. We've built and deployed a prototype - the implementation has taken a number of shortcuts, but I'll focus on the underlying principles.
[slide 3]

David: primary a question about who should have what powers, and a demonstration that it can be given
[slide 4] shows famous misinformation flood cases, incl "review bombing"
[slide 5]

David: Generally governments and NGOs ask platforms to fix mis-information, but we argue they can't because of scale and because vast amounts of it appears off-platform; and platforms themselves aren't necessarily trusted to do so, esp when the notion of misinformation itself has no clear consensus definition

[slide 6]

David: Trust is our primary source of knowledge (a very tiny proportion comes from our own experience or research) - social construction of knowledge
[slide 7]

David: TrustNet takes a trust-centric approach - allowing anyone to assess accuracy of any content, and give autonomy to users to decide who to trust the assessment of (fitting the role of "facilitators")
[slide 8]

David: TrustNet is a web site and a browser extension, allowing anyone to make a post or a web page as accurate or inaccurate; people trusting that facilitator will see that mark on the page itself, and on pages linking to it (since that's already a factor or assessment), and fading links to untrusted content

[slide 9]

David: two different networks in TrustNet: Trust network and Follow Network - the two don't necessarily overall (e.g. crazy relatives, untrustworthy politician)

[slide 10] shows TrustNet extension in use to mark content or accurate or inaccurate

[slide 11] shows rendering of that mark on a news web site

[slide 12] shows rendering on a social network - currently limited to specific sites due to lack of standards for broader integration

[slide 13]

David: a second browser extension focuses on "headline", a critical component of misinformation since articles are often more accurate than their headlines; this offers to fix misinforming headlines which will appear to people that trust you. Headlines are identified through heuristics

[slide 14] shows this in action on the NYTimes

Note: Reheadline extension is “This extension is no longer available because it doesn't follow best practices for Chrome extensions.”

[slide 16]

David: a regular objection is around scale - not enough sources to assess the entire Web; we expect propagation could help (transitive trust); not done yet since not clear how to do that properly - but a great possible additional subject of research

[slide 17]

David: based on surveys, we've seen a lot of interest from people to take on that role, incl fact checkers and journalists; ti also already happens in comments (where it tends to be buried);

[slide 18]

David: another concern is around filter bubbles, e.g. conspiracy theorists strengthening one another; there is an ethical argument to say people deserve autonomy in making possibly wrong choices; and practically, you can't force people to change their trust network - conversely, exposing discrepancies in assessment from your trust network can help break through these bubbles

[slide 19]

David: this work started before the AI renaissance; AI is making this worse. A lot of our usual assessment heuristics are doomed in a world where AI can generate a lot more credible looking content - that makes trust a much more prominent factor in building our own assessment

[slide 20]

David: AI may even need us to reverse the usual basic assumption that most content is trustworthy

[slide 21]

David: AI can also help scaling fact checking, incl through re-headlining

[slide 23]
David: lots of open questions on infrastructure; also questions on privacy (should assessments be public or private?); how many assessment repositories should we expect (a few or many)

[slide 24]

David: from a browser perspective, we probably want to attach assessment to piece of content on a Web page, not on a page as a whole

[slide 25]

Rick: I work on Chrome, we've been concerned about this problem for a long time; we've been wondering what it would take to show trust signal in the Chrome UI. We had concerns about finding the right incentives in the user interest. We identified a list of properties we want to see met to invest in this <https://docs.google.com/document/d/1wTFafdHa-o3OYCKmYzEJGROrpSoxXN6DNXPltzdiUzg> - this feels like this meets exactly what we've been looking for; clearly this isn't something that Google would want to take responsibility for - we've been struggling already with certificate authorities, we couldn't scale the centralized (CA-style) type of approach to something that needs billions of users. We're really excited about this direction.

Leonard: can you say a bit more about "accuracy" as a target vs "real" or "authentic"? How does that apply to AI-generated material labeled vs unlabeled?

Farnaz: we have another paper exactly on that; labelling things as real or inauthentic can create real harms (e.g. edited images aren't necessarily misinformation). There are other dimensions of credibility beyond accuracy - they could also be captured, but we've focused on the accuracy signal at the moment.

David: "true but misleading" is an example of something where "accuracy" doesn't help. We've focused on one bit in this prototype to avoid drowning in complexity, but this definitely further exploration. We have another paper exploring taxonomy on what people use to determine something is true.

Leonard: so "accuracy" is "truth as perceived by the reviewer"?

David: there will be as many definitions or perception of what accurate means as end users

Jeffrey: that's part of what I would expect the system to evolve to - each end user has a particular goal with the system, without necessarily thinking hard about what that means, but they'll find facilitators that surface the signal they want.

David: a lot of work around whether something has been AI-generated or not - it feels secondary to me; accuracy feels more important than the creator

Leonard: a key point is that these are signals - I worry a bit about how this scales to more signals beyond accuracy

David: in terms of usability, I don't think we can ask users to do too much (both as facilitator or assessment consumer); I expect more signals in the system, but not necessarily expose them all to all users - a binary red/green-like signal is much more likely to be the level of information most users would want to see (with possibly to dive down)

Annette: it will boil down to what the UI surfaces

David: I don't want this to be confused with upvote/downvote - they signal agreements rather than accuracy, which should be able to be signaled differently

Farnaz: another signal we show - if only one has person has given a different assessment, that is surfaced as "disputed"

Franck: I like this idea of crowdsourcing trust; what kind of impact you expect this to have compared to the scale of misinformation flood, supported by engagement-driven metrics (with fake/biased news generating a lot more engagement).

Farnaz: misinformation is multi-faceted, so this is addressing only one aspect naturally; but we're also showing signals of content that has not been assessed

David: in general, users do look for content that trigger emotions; but conversely, most of us don't want to be fooled. Platforms may not have incentives to support truth (they have a conflict interest) - in that sense, trustNet is a hostile intervention for them. Users of trustNet can configure their filters (e.g. hide things assessed as false; or hide things not assessed as true) - goal to steer society in a direction where no signal means "garbage".

Franck: this would work for browsers - but would this work within apps?

David: this works for PWAs; for native apps, no easy solution since users don't have as much autonomy (see "the three legged stools" re loyal clients); I want to create pressure from loyal apps to get more apps to adopt that approach

Scott: re your concern around widespread manipulation - e.g. a big influencer trying to shift accuracy ratings of a particular content

David: we're in an asymmetric situation where we've seen failures from centralized approaches; we can imagine things going wrong from decentralized approaches, but they need to be explored and tried before abandoning them. To your particular issue, the benefit of TrustNet is that allows auditing - people can change their assessment, but that is recorded which others can verify to make their choices

Sarven: compared to social media: TrustNet is centralized with users expected to create accounts and store their assessment - in that sense, similar to YouTube. Beyond that, how do you factor domain expertise, degree of relatedness?

David: re centralization - as a prototype, TrustNet is a centralized platform as a shortcut to allow to demonstrate the UI/UX. I don't think that's the right way to do things, and I don't think it needs to be centralized. We should integrate existing identity and provenance standards. There will likely be a centralized component (e.g. to crawl content) - although even that could be decentralized based on trust relationships. With regard to domain/topic expertise - clearly there are people you trust on insights e.g. on health issues but not at all on politics. TrustNet doesn't allow for that granularity at that moment for sake of adoption.

Farnaz: we do have some proxy for domain expertise via source lists; e.g. this is a list of people I trust on @@@

David: but it's not exposed in the UI

Dom: Big Privacy red flag. How do we use this without revealing too much about ourselves/the people we trust? How many of these big open questions (privacy, centralization) do you see remaining before we consider scaling up to creating a standard? You might want to assess a piece of a website. Who decides how to chunk a website?

David: we had lots of discussion on privacy at the beginning of TrustNet: 3 types of risks:

- Who I trust

- my assessments

- My queries

TrustNet keeps private who I trust, make assessments public, and doesn't say anything about queries - there are techniques to anonymize we started discussing.

Keeping trust private feels important given potential social pressure - making it public would make e.g. propagation much easier.

In terms of open questions and which needs tackling before getting something out - lots of questions needs answering, but they shouldn't stop from scaling up experimentation.

ChrisN: re accuracy/inaccuracy - how much risk you see this evolve in "this fits my political view"?

David: a lot of people will use it to express agreement rather than accuracy - hence the value of auditing

ChrisN: any analysis of risk of this becoming a discussion forum?

David: I'm doing a lot of research on annotations, incl in the content of discussion forum; but TrustNet doesn't expose this in the UI at the moment

Farnaz: there are opportunities to hold people's hand in the assessment process to make it clear they're not asked to express agreement; see also the other paper we mentioned where people are asked to explain why they're making the assessment

Jennifer: could people have their own set of settings of how they see trust? Trust is subjective, temporal - having a way for people to express their own principles to build their tool. How can we help make/follow progress?

David: we're interested in any support that can be provided to scale this to impact the world; we need help to do it. Great opportunity that this meeting helped establish. In terms of defining trust mechanisms - we only capture people to identify their facilitators, not why.

From Jeffrey: We should also think about what to do next. I'd like to sketch out how to 1) let facilitators publish assessments independent of the centralized aggregator, 2) let multiple aggregators share those assessments so we can 3) let consumers choose the aggregator they trust to protect their privacy. (Is this all ActivityPub?) Also, 4) how do we identify the pieces of HTML that are "links"? Sub-articles are probably \<article>.

Tzviya: More projects are upcoming in this series, although we'll need to figure out when to migrate to a F2F mode; great connections being made across projects too :)
